{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Web Traffic Analysis\n",
    "**This is the second of three mandatory projects to be handed in as part of the assessment for the course 02807 Computational Tools for Data Science at Technical University of Denmark, autumn 2019.**\n",
    "\n",
    "#### Practical info\n",
    "- **The project is to be done in groups of at most 3 students**\n",
    "- **Each group has to hand in _one_ Jupyter notebook (this notebook) with their solution**\n",
    "- **The hand-in of the notebook is due 2019-11-10, 23:59 on DTU Inside**\n",
    "\n",
    "#### Your solution\n",
    "- **Your solution should be in Python**\n",
    "- **For each question you may use as many cells for your solution as you like**\n",
    "- **You should document your solution and explain the choices you've made (for example by using multiple cells and use Markdown to assist the reader of the notebook)**\n",
    "- **You should not remove the problem statements**\n",
    "- **Your notebook should be runnable, i.e., clicking [>>] in Jupyter should generate the result that you want to be assessed**\n",
    "- **You are not expected to use machine learning to solve any of the exercises**\n",
    "- **You will be assessed according to correctness and readability of your code, choice of solution, choice of tools and libraries, and documentation of your solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this project your task is to analyze a stream of log entries. A log entry consists of an [IP address](https://en.wikipedia.org/wiki/IP_address) and a [domain name](https://en.wikipedia.org/wiki/Domain_name). For example, a log line may look as follows:\n",
    "\n",
    "`192.168.0.1 somedomain.dk`\n",
    "\n",
    "One log line is the result of the event that the domain name was visited by someone having the corresponding IP address. Your task is to analyze the traffic on a number of domains. Counting the number of unique IPs seen on a domain doesn't correspond to the exact number of unique visitors, but it is a good estimate.\n",
    "\n",
    "Specifically, you should answer the following questions from the stream of log entries.\n",
    "\n",
    "- How many unique IPs are there in the stream?\n",
    "- How many unique IPs are there for each domain?\n",
    "- How many times was IP X seen on domain Y? (for some X and Y provided at run time)\n",
    "\n",
    "**The answers to these questions can be approximate!**\n",
    "\n",
    "You should also try to answer one or more of the following, more advanced, questions. The answers to these should also be approximate.\n",
    "\n",
    "- How many unique IPs are there for the domains $d_1, d_2, \\ldots$?\n",
    "- How many times was IP X seen on domains $d_1, d_2, \\ldots$?\n",
    "- What are the X most frequent IPs in the stream?\n",
    "\n",
    "You should use algorithms and data structures that you've learned about in the lectures, and you should provide your own implementations of these.\n",
    "\n",
    "Furthermore, you are expected to:\n",
    "\n",
    "- Document the accuracy of your answers when using algorithms that give approximate answers\n",
    "- Argue why you are using certain parameters for your data structures\n",
    "\n",
    "This notebook is in three parts. In the first part you are given an example of how to read from the stream (which for the purpose of this project is a remote file). In the second part you should implement the algorithms and data structures that you intend to use, and in the last part you should use these for analyzing the stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the stream\n",
    "The following code reads a remote file line by line. It is wrapped in a generator to make it easier to extend. You may modify this if you want to, but your solution should remain parametrized, so that your notebook can be run without having to consume the entire file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "def stream(n):\n",
    "    i = 0\n",
    "    with urllib.request.urlopen('https://files.dtu.dk/fss/public/link/public/stream/read/traffic_2?linkToken=_DcyO-U3MjjuNzI-&itemName=traffic_2') as f:\n",
    "        for line in f:\n",
    "            element = line.rstrip().decode(\"utf-8\")\n",
    "            yield element\n",
    "            i += 1\n",
    "            if i == n:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "STREAM_SIZE = 10000\n",
    "web_traffic_stream = stream(STREAM_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "\n",
    "---\n",
    "\n",
    "The libraries we use here are numpy and mmh3.\n",
    "\n",
    "Numpy is used for better matrix manipulation, and mmh3's hash function is used to have an easy way to uniformly distribute the IPs coming from the stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import mmh3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting how many different IPs are there in the stream :\n",
    "\n",
    "In order to solve the question on how many uniques IPs are there in the stream, we resort to the HyperLogLog Sketch which is specifically designed to count distinct elements. By using the bit representation of the input, slicing it and updating one of the _m_ counters. It is useful due to its speed and accuracy.\n",
    "\n",
    "The estimation of the unique IPs, have a standard deviation = 1.04E/m^0.5 which, if we take the answer from a stream_size of 100000, the answer is 85864.85, so the expected value is around the  (74.701,68, 97.026,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the position of the first 1 bit from the left in the word until endIndex\n",
    "def get_index(binary):\n",
    "    pos = -1\n",
    "    try:\n",
    "        pos = binary.index('1')+1\n",
    "    except(ValueError):\n",
    "        pos = endIndex\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The uniqueIPs function takes as input the stream and then it calculates the values to slice the stream (represented in binary) acording to the given m , then it starts a counter of size length m . For the alpha values we took the suggested in the paper from Flajolet (https://hal.archives-ouvertes.fr/file/index/docid/406166/filename/FlFuGaMe07.pdf ), same with the size adjustments to E . Note that we did not calculate the harmonic mean as stated on the lecture, but we used E = alpha*m^2*(sum(2^M[j])-1). To start assigning values and thus counting, the function splits the line in the stream , taking just the IP, then hashes it into binary, then checks that the binary is of 32 bits, and then slices it into a lower and upper part. Then gets the index of the left-most bit of the lower part of the bit, after it assigns the value of M to update between the max of the last M recorded at that position or the p value. Finally, you get E, with the formula described before and adjustments to this E value are made depending on the selection of m (this selection depends on the application that it will be used and the number of expected distinct elements)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniqueIPs(stream, m=64, w=64, seed=42):\n",
    "    \n",
    "    # Transforming the data\n",
    "    upper_m = int(numpy.log2(m))\n",
    "    upperval=int(numpy.power(2,upper_m)-1)\n",
    "    M = numpy.zeros(m)\n",
    "    alpha=0\n",
    "    if (m<32):\n",
    "        alpha=0.673\n",
    "    elif (m<64):\n",
    "        alpha=0.697    \n",
    "    elif (m<128):\n",
    "        alpha=0.709\n",
    "    else:\n",
    "        alpha=0.7213/(1+ 1.079/m)\n",
    "    \n",
    "    # Iterating the stream\n",
    "    for line in stream:\n",
    "        log = line.split()\n",
    "        hashy = mmh3.hash(log[0], seed=seed, signed=False) \n",
    "        bin_hash = bin(hashy)[2:]\n",
    "        while len(bin_hash)<32:\n",
    "            bin_hash='0'+bin_hash\n",
    "        upper = bin_hash[:upper_m]\n",
    "        lower = bin_hash[upper_m:]\n",
    "        p = get_index(lower)\n",
    "        j = int(upper, 2)\n",
    "        M[j] = max(M[j], p)\n",
    "        \n",
    "    # Getting the value\n",
    "    suma=0\n",
    "    for j in range(m):\n",
    "        suma+=numpy.power(2,-M[j])\n",
    "\n",
    "    E = alpha*numpy.power(m,2)*numpy.power(suma,-1)\n",
    "    \n",
    "    if (E<=5*m/2):\n",
    "        V=numpy.count_nonzero(M==0)\n",
    "        if (V!=0):\n",
    "            E_a=m*numpy.log(m/V)\n",
    "        else:\n",
    "            E_a=E\n",
    "    elif (E<=(numpy.power(2,32)/30)):\n",
    "        E_a=E\n",
    "    else:\n",
    "        E_a=-2*numpy.log(1-E/numpy.power(2,32))\n",
    "        \n",
    "    return E_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are aprox. nan unique IPs in the stream.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programmes\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:46: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "D:\\Programmes\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:46: RuntimeWarning: invalid value encountered in log\n"
     ]
    }
   ],
   "source": [
    "uniq_ips = uniqueIPs(web_traffic_stream,m=32)\n",
    "print(\"There are aprox. \"+str(uniq_ips)+\" unique IPs in the stream.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting how many different IPs for each domain are there in the stream :\n",
    "\n",
    "To count the unique IPs seen on each of the domains we have to implement again the HyperLogLog algorithm but instead of having just M counters, we will have M arrays of counters for the distinct elements on each of the domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting how many times a certain IP visited a certain domain :\n",
    "\n",
    "   To know how many times an IP visited a domain, we firstly need to build a data structure that allows us to know how much time each IP visited each domain. \n",
    "   This data structure will cause the query to be very efficient, as we will only need to go through a slim part of the data structure to answer the query.\n",
    "   \n",
    "   Here, we use the data structure of a CountMin sketch, as it is an efficient way to count the frequency of elements in a stream without storing all the elements.\n",
    "   The main issue we fall into when implementing this sketch is that here, we have an unknown number of domains, where the CountMin squetch is not initially made to differentiate the elements. \n",
    "   What we did to solve this problem is that we created a dictionnary that relates the domain name to its own CountMin Data Structure.\n",
    "   \n",
    "   Given that we work with a stream, we had to increment the dictionnary on the go with new domains every time they appear to solve this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IP_frequency(stream, d, w):\n",
    "    domains_dict = {}\n",
    "    for i in stream:\n",
    "        current_element = i.split()\n",
    "        current_IP = current_element[0]\n",
    "        current_domain = current_element[1]\n",
    "        if current_domain in domains_dict:\n",
    "            for j in range(d):\n",
    "                hashw = mmh3.hash(current_IP, j) % w\n",
    "                domains_dict[current_domain][j, hashw] += 1\n",
    "        else:\n",
    "            domains_dict[current_domain]= numpy.zeros((d,w))\n",
    "            for j in range(d):\n",
    "                hashw = mmh3.hash(current_IP, j) % w\n",
    "                domains_dict[current_domain][j, hashw] += 1\n",
    "    return domains_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a working Data Structure, we want to have a look at it and see how it interacts with the different parameters : the imput stream size we chose, the number of rows in our matrix and the number of values our hash function can take:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'python.org': array([[0., 0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 2., 1.]]),\n",
       " 'wikipedia.org': array([[0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 2., 0., 0., 0., 0.],\n",
       "        [0., 2., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.]]),\n",
       " 'pandas.pydata.org': array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IP_Frequency_Test_Stream = stream(10)\n",
    "IP_frequency(IP_Frequency_Test_Stream, 4, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining the query**\n",
    "\n",
    "Now that we have our database, we can go and use it to answer our initial question : How many time was a certain IP seen on a certain domain ?\n",
    "\n",
    "To do that, we go through each line of the matrix associated to our domain and build a list out of the values from our ip. The functiun then returns the minimum of this value, as originaly intended in the CountMin squetch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def CountMin(stream, ip, domain, d, w):\n",
    "    Count_List = []\n",
    "    Domains_Dict = IP_frequency(stream, d, w)\n",
    "    if domain in Domains_Dict:\n",
    "        M = Domains_Dict[domain]\n",
    "        for k in range(d):\n",
    "            Count_List.append(M[k, mmh3.hash(ip, k) % w])\n",
    "    else:\n",
    "        Count_List.append(0)\n",
    "    return min(Count_List)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to check the accuracy of our query, we can use a function that will have 100% accuracy while being absolutely inefficient :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accurate_IP_Count(stream, ip, domain):\n",
    "    count = 0\n",
    "    for i in stream:\n",
    "        current_element = i.split()\n",
    "        current_IP = current_element[0]\n",
    "        current_domain = current_element[1]\n",
    "        if (current_IP == ip) & (current_domain == domain):\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have an idea about the result and the accuracy we can expect from out squetch, we firstly made a blind try with arbitrary values :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimated number of times Test_IP went on Test_Domain is :\n",
      "3.0\n",
      "The exact number of times Test_IP went on Test_Domain is :\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "Test_IP = '202.152.82.171'\n",
    "Test_Domain = 'wikipedia.org'\n",
    "CountMin_Test_Stream = stream(STREAM_SIZE)\n",
    "\n",
    "print('The estimated number of times Test_IP went on Test_Domain is :')\n",
    "print(CountMin(CountMin_Test_Stream, Test_IP, Test_Domain, 10,1000))\n",
    "\n",
    "CountMin_Check_Stream = stream(STREAM_SIZE)\n",
    "\n",
    "print('The exact number of times Test_IP went on Test_Domain is :')\n",
    "print(Accurate_IP_Count(CountMin_Check_Stream, Test_IP, Test_Domain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, this first answer does not seem to be even close from the result we expected. We now need to look for the best values we can give $d$ and $w$ in order to have the highest chance of getting the most precise value.\n",
    "\n",
    "According to *An Improved Data Stream Summary:The Count-Min Sketch and its Applications*, the number of values our hash function can take, $w$, is tied to $\\epsilon$, the factor defining the error, and $d$ is linked to $\\delta$, the probability of the error being in that $\\epsilon$ factor.\n",
    "\n",
    "Finding a correct value for $d$ is a rather easy task. We have : \n",
    "\n",
    "$d=\\ln(\\frac{1}{\\delta})$ so, if we want the error rate to be less than 0.01%, we find that $d$ should be at least 9.22, thus rounding up $d$ to $d=10$ fits the probability we asked for.\n",
    "\n",
    "To find the value $\\epsilon$ that fits our needs, we need to have a deeper reflexion than simply asking us what the probability $\\delta$ we want.\n",
    "Indeed, with each iteration of the stream, the error cumulates, and so we need to account for that when we choose our $w$ dimension. We have :\n",
    "\n",
    "* $f_e \\leq f'_e \\leq f_e \\cdot \\epsilon \\cdot N$, with $f_e$ the real frequency, $f'_e$ the estimated frequency and $N$ the number of items we have read in the stream.\n",
    "\n",
    "* $w = \\frac{e}{\\epsilon}$\n",
    "\n",
    "Thus, $f_e \\leq f'_e \\leq f_e \\cdot \\frac{e}{w} \\cdot N$ \n",
    "\n",
    "And thus we can chose w depending on the number of items in the stream, and the error interval we want. Tipically, if you want your approximation to be at most 10% off of the real value, you want to chose $w$ such that $\\frac{e\\cdot N}{w} = 1.1$, which gives us $w \\approx 2.5 \\cdot N$\n",
    "\n",
    "Of course, building a Data Structure that is bigger than the stream makes no sense as using streaming algorithms would be pointless, so we want to take $w$ as a fraction of $N$, depending on our precesion needed, our processing time available and our memory available.\n",
    "\n",
    "For that, it seemed that $w = \\frac{N}{10}$ could be a good compromise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting how many times a certain IP visited a list of domains:\n",
    "\n",
    "To implement this query, we use the same Data Structure as for a single domain, but the query is a bit more complex :\n",
    "\n",
    "We use the fact that indepently of the domain, hashing an IP will always give the same result, and so adding up the values from each domain line per line, or hash seed by hash seed, allows us to minimize the error by not counting it every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Count_Multiple_Min(stream, ip, domains, d, w):\n",
    "    Count_List = []\n",
    "    Domains_Dict = IP_frequency(stream, d, w)\n",
    "    for k in range(d):\n",
    "        Sum = 0\n",
    "        for d in domains:\n",
    "            if d in Domains_Dict:\n",
    "                Sum += Domains_Dict[d][k, mmh3.hash(ip, k) % w]\n",
    "# For clarity purpose, one could add this condition:\n",
    "            #else:\n",
    "            #    Sum += 0\n",
    "        Count_List.append(Sum)\n",
    "    return min(Count_List)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, doing a step-by-step analysis to calculate the parameters would be a rather complicated task, but we could argue that having limited the times we add the error, the final error could be proportionnal to the individual error.\n",
    "Thus, taking the same parameters as in the single-domain query would lead to an error that, even if bigger, still corresponds to the magnitude of the single-domain error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Multiple_Domains_Stream = stream(STREAM_SIZE)\n",
    "Test_Domains = ['wikipedia.org','python.org','lol.lol','github.com']\n",
    "\n",
    "Count_Multiple_Min(Multiple_Domains_Stream, Test_IP, Test_Domains, 10, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having tried to find how many times IP X went on a few differents domains, we have found that when accounting error rates, every single IP we have tried appeared less than just a few times, meaning that those queries are, given this stream set, not that usefull since it does not give any valuable information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
